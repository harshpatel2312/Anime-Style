{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98498810-85fb-464f-bc0b-35c32730eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from losses import GANLoss\n",
    "from dataloader import get_dataloader\n",
    "from itertools import cycle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9605ad12-1ea5-4888-b7ca-bfdb3a3c3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 2\n",
    "lr = 2e-4\n",
    "log_interval = 10 # For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5662bd-80e6-40b7-90a6-e0d6198c7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data folders\n",
    "anime_train = \"../data/anime/train/\"\n",
    "real_train = \"../data/real/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a9091b-924c-45ca-a060-638e4048927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATALOADERS ==========\n",
    "anime_train_loader, real_train_loader = get_dataloader(anime_train, real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafd6436-608b-411a-bdb7-6fdabbc9a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODELS ==========\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b744585-857f-40fc-ba47-ba91a54283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LOSS FUNCTIONS ==========\n",
    "criterion_gan = GANLoss(use_mse = True).to(device) # Adversarial Loss\n",
    "criterion_l1 = nn.L1Loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac6d44e-38f3-46a2-8104-96e497dfaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== OPTIMIZERS ==========\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a7c03-b0fd-47db-a246-35354f10c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2] [Batch 10] Loss_D: 0.6522 | Loss_G: 3.2036\n",
      "[Epoch 1/2] [Batch 20] Loss_D: 0.6632 | Loss_G: 3.7382\n",
      "[Epoch 1/2] [Batch 30] Loss_D: 3.3488 | Loss_G: 3.3244\n"
     ]
    }
   ],
   "source": [
    "# ========== TRAINING ==========\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Set to training mode\n",
    "    G.train() \n",
    "    D.train()\n",
    "\n",
    "    epoch_loss_G = 0\n",
    "    epoch_loss_D = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (real_imgs, anime_imgs) in enumerate(zip(cycle(real_train_loader), anime_train_loader)):\n",
    "        if i >= len(anime_train_loader):\n",
    "            break\n",
    "            \n",
    "        real_imgs = real_imgs.to(device)\n",
    "        anime_imgs = anime_imgs.to(device)\n",
    "\n",
    "        # ========================================\n",
    "        # 1. Train Discriminator\n",
    "        # ========================================\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_anime = G(real_imgs).detach() # Exclude gradients\n",
    "        pred_real = D(anime_imgs) # Predict real\n",
    "        pred_fake = D(fake_anime) # Predict fake\n",
    "\n",
    "        loss_D_real = criterion_gan(pred_real, True) # Calculate adversarial loss \n",
    "        loss_D_fake = criterion_gan(pred_fake, False) # Calculate recnstruction loss\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5 # Total loss\n",
    "        loss_D.backward() # Backpropogation\n",
    "        optimizer_D.step() # Update weights\n",
    "\n",
    "        # ========================================\n",
    "        # 2. Train Generator\n",
    "        # ========================================\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_anime = G(real_imgs) # Convert real -> anime\n",
    "        pred_fake_for_G = D(fake_anime) # Step to fool dicriminator in identifying fake as real...\n",
    "        loss_G_GAN = criterion_gan(pred_fake_for_G, True) # Calculate adversarial loss \n",
    "        loss_G_L1 = criterion_l1(fake_anime, anime_imgs) # Compare both original and newly created image\n",
    "        loss_G = loss_G_GAN + 10.0 * loss_G_L1 # Total loss\n",
    "        loss_G.backward() # Backpropogation\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ========================================\n",
    "        # Logging\n",
    "        # ========================================\n",
    "        epoch_loss_G += loss_G.item()\n",
    "        epoch_loss_D += loss_D.item()\n",
    "\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i+1}] \"\n",
    "                  f\"Loss_D: {loss_D.item():.4f} | Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # === Epoch Summary ===\n",
    "    num_batches = len(anime_train_loader)\n",
    "    print(f\"â†’ Epoch {epoch} Done | Time: {time.time() - start_time:.1f}s | \"\n",
    "          f\"Avg Loss_D: {epoch_loss_D / num_batches:.4f} | \"\n",
    "          f\"Avg Loss_G: {epoch_loss_G / num_batches:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1af19-445d-48f8-9732-c1f288b9c9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a7486-1a06-4833-bf0a-e296f5279f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68445e-3c7a-4705-b5e4-532a6545c2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28656f23-2e8a-4f16-9f45-24976abeb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0079d1-3e4a-427f-906f-7e6a6a15ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_train_loader, real_train_loader = get_dataloader(anime_train, real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3aed63-3e8f-4abf-98ed-ee72f2ac0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# batch = next(iter(anime_train_loader))\n",
    "# image_tensor = batch[0]\n",
    "# image_tensor = (image_tensor + 1) / 2\n",
    "# pil_img = to_pil_image(image_tensor)\n",
    "# pil_img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
